<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine-Learning | Haihao Sun&#39;s Homepage</title>
    <link>http://localhost:1313/tags/machine-learning/</link>
      <atom:link href="http://localhost:1313/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine-Learning</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sat, 19 Apr 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu68170e94a17a2a43d6dcb45cf0e8e589_3079_512x512_fill_lanczos_center_3.png</url>
      <title>Machine-Learning</title>
      <link>http://localhost:1313/tags/machine-learning/</link>
    </image>
    
    <item>
      <title>D2L - Chapter 3.1 Linear Regression &#43; 3.2 Documentation</title>
      <link>http://localhost:1313/post/2025-04-19-chapter3-1&#43;2-linear-regression/</link>
      <pubDate>Sat, 19 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2025-04-19-chapter3-1&#43;2-linear-regression/</guid>
      <description>&lt;h2 id=&#34;31-linear-regression&#34;&gt;3.1 Linear Regression&lt;/h2&gt;
&lt;h3 id=&#34;311-basics&#34;&gt;3.1.1 Basics&lt;/h3&gt;
&lt;h4 id=&#34;3111-model&#34;&gt;3.1.1.1 Model&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;y = wx + b&lt;/li&gt;
&lt;li&gt;search for the best w and b&lt;/li&gt;
&lt;li&gt;measure of the quality of some given model (todo - 1)&lt;/li&gt;
&lt;li&gt;a procedure for updating the model to improve its quality (todo - 2)&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;3112-loss-function&#34;&gt;3.1.1.2 Loss Function&lt;/h4&gt;
$$
L(\mathbf{w}, b)
      = \frac{1}{n} \sum_{i=1}^{n} l^{(i)}(\mathbf{w}, b)
      = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{2}\!\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^{2}. \tag{3.1.6}
$$
&lt;h4 id=&#34;3113-analytic-solution&#34;&gt;3.1.1.3 Analytic Solution&lt;/h4&gt;
$$
\partial_{\mathbf{w}} \| \mathbf{y} - \mathbf{Xw} \|^2 = 2 \mathbf{X}^\top (\mathbf{Xw} - \mathbf{y}) = 0
\quad \text{and hence} \quad
\mathbf{X}^\top \mathbf{y} = \mathbf{X}^\top \mathbf{Xw}. \tag{3.1.8}
$$
&lt;p&gt;Solving for $\mathbf{w}$ provides us with the optimal solution for the optimization problem. Note that this solution&lt;/p&gt;
$$
\mathbf{w}^* = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \tag{3.1.9}
$$
&lt;h4 id=&#34;3114-minibatch-stochastic-gradient-descent&#34;&gt;3.1.1.4 Minibatch Stochastic Gradient Descent&lt;/h4&gt;
</description>
    </item>
    
    <item>
      <title>D2L - Chapter 2.6 Probablity and Statistics &#43; 2.7  Documentation</title>
      <link>http://localhost:1313/post/2025-04-12-chapter2-6&#43;7-probabilty-and-statistics&#43;documentation/</link>
      <pubDate>Sat, 12 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2025-04-12-chapter2-6&#43;7-probabilty-and-statistics&#43;documentation/</guid>
      <description>&lt;h2 id=&#34;probablity-and-statistics&#34;&gt;Probablity and Statistics&lt;/h2&gt;
&lt;h3 id=&#34;covariance-matrix-diagonal--off-diagonal-entries&#34;&gt;Covariance Matrix, Diagonal &amp;amp; Off-Diagonal Entries&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Covariance Matrix&lt;/strong&gt;&lt;br&gt;
A covariance matrix \(\Sigma\) is a square matrix that shows the covariance between each pair of coordinates in a random vector. It is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Symmetric&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Positive semidefinite&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Diagonal Entries&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each diagonal entry \(\Sigma\_{ii}\) is the &lt;strong&gt;variance&lt;/strong&gt; of the \(i\)-th coordinate, denoted \(\mathrm{Var}(x_i)\).&lt;/li&gt;
&lt;li&gt;Variance measures how spread out the \(i\)-th coordinate is around its mean.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Off-Diagonal Entries&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each off-diagonal entry \(\Sigma\_{ij}\) (with \(i \neq j\)) is the &lt;strong&gt;covariance&lt;/strong&gt; \(\mathrm{Cov}(x_i, x_j)\).&lt;/li&gt;
&lt;li&gt;Positive covariance means the two coordinates tend to vary in the same direction (above/below their means together), negative covariance indicates they move in opposite directions, and zero covariance indicates no linear co-movement.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Why Symmetric?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Because \(\mathrm{Cov}(x_i, x_j) = \mathrm{Cov}(x_j, x_i)\), the matrix \(\Sigma\) must be symmetric.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;documentation&#34;&gt;Documentation&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;dir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;jax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;help&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;jax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;numpy&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ones&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>D2L - Chapter 2.4 Calculus &#43; 2.5 Automatic Differentiation</title>
      <link>http://localhost:1313/post/2025-04-09-d2l-chapter2-4&#43;5-calculus&#43;automatic-differentiation/</link>
      <pubDate>Wed, 09 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2025-04-09-d2l-chapter2-4&#43;5-calculus&#43;automatic-differentiation/</guid>
      <description>&lt;h2 id=&#34;24-calculus&#34;&gt;2.4 Calculus&lt;/h2&gt;
&lt;h3 id=&#34;241-derivatives-and-differentiation&#34;&gt;2.4.1 Derivatives and Differentiation&lt;/h3&gt;
&lt;h3 id=&#34;242-visualization-utilities&#34;&gt;2.4.2 Visualization Utilities&lt;/h3&gt;
&lt;h3 id=&#34;243-partial-derivatives-and-gradients&#34;&gt;2.4.3 Partial Derivatives and Gradients&lt;/h3&gt;
&lt;p&gt;When there is no ambiguity, ∇ₓf(&lt;strong&gt;x&lt;/strong&gt;) is typically replaced by ∇f(&lt;strong&gt;x&lt;/strong&gt;). The following rules come in handy for differentiating multivariate functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For all &lt;strong&gt;A&lt;/strong&gt; ∈ ℝᵐˣⁿ we have ∇ₓ(&lt;strong&gt;A&lt;/strong&gt;x) = &lt;strong&gt;A&lt;/strong&gt;ᵀ and ∇ₓ(&lt;strong&gt;x&lt;/strong&gt;ᵀ&lt;strong&gt;A&lt;/strong&gt;) = &lt;strong&gt;A&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;For square matrices &lt;strong&gt;A&lt;/strong&gt; ∈ ℝⁿˣⁿ we have that ∇ₓ(&lt;strong&gt;x&lt;/strong&gt;ᵀ&lt;strong&gt;A&lt;/strong&gt;x) = (&lt;strong&gt;A&lt;/strong&gt; + &lt;strong&gt;A&lt;/strong&gt;ᵀ)&lt;strong&gt;x&lt;/strong&gt; and in particular&lt;br&gt;
∇ₓ‖&lt;strong&gt;x&lt;/strong&gt;‖² = ∇ₓ(&lt;strong&gt;x&lt;/strong&gt;ᵀ&lt;strong&gt;x&lt;/strong&gt;) = 2&lt;strong&gt;x&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;244-chain-rule&#34;&gt;2.4.4 Chain Rule&lt;/h3&gt;
&lt;h3 id=&#34;245-discussion&#34;&gt;2.4.5 Discussion&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Automatic differentiation&lt;/strong&gt; allows us to compute gradients systematically using composition rules, freeing up cognitive effort for higher-level tasks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gradient computation&lt;/strong&gt; for vector-valued functions involves matrix multiplication, following the chain of variable dependencies.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dependency graphs&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Evaluated &lt;strong&gt;forward&lt;/strong&gt; to compute function outputs.&lt;/li&gt;
&lt;li&gt;Traversed &lt;strong&gt;backward&lt;/strong&gt; to compute gradients (backpropagation).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Backpropagation&lt;/strong&gt; is a computational application of the chain rule, introduced formally in later chapters.&lt;/li&gt;
&lt;li&gt;In &lt;strong&gt;optimization&lt;/strong&gt;, gradients indicate how to adjust model parameters to reduce the loss.&lt;/li&gt;
&lt;li&gt;Every step of an optimization algorithm relies on computing the gradient.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;25-automatic-differentiation&#34;&gt;2.5 Automatic Differentiation&lt;/h2&gt;
&lt;h3 id=&#34;251-a-simple-function&#34;&gt;2.5.1 A simple function&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;jax&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;grad&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arange&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;4.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;lambda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# The `grad` transform returns a Python function that&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# computes the gradient of the original function&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;x_grad&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;grad&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;x_grad&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;252-backward-for-non-scalar-variables&#34;&gt;2.5.2 Backward for Non-Scalar Variables&lt;/h3&gt;
&lt;h3 id=&#34;253-detaching-computation&#34;&gt;2.5.3 Detaching Computation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;move some calculations outside the recorded computational graph&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;254-gradients-and-python-control-flow&#34;&gt;2.5.4. Gradients and Python Control Flow&lt;/h3&gt;
&lt;h3 id=&#34;255-discussion&#34;&gt;2.5.5 Discussion&lt;/h3&gt;
&lt;p&gt;For now, try to remember these basics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(i) attach gradients to those variables with respect to which we desire derivatives;&lt;/li&gt;
&lt;li&gt;(ii) record the computation of the target value;&lt;/li&gt;
&lt;li&gt;(iii) execute the backpropagation function;&lt;/li&gt;
&lt;li&gt;(iv) access the resulting gradient.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>D2L - Chapter 2.2 Data Preprocessing &#43; 2.3 Linear Algebra</title>
      <link>http://localhost:1313/post/2025-04-08-d2l-chapter2-2&#43;3-data-preprocessing&#43;linear-algebra/</link>
      <pubDate>Tue, 08 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2025-04-08-d2l-chapter2-2&#43;3-data-preprocessing&#43;linear-algebra/</guid>
      <description>&lt;h2 id=&#34;22-data-preprocessing&#34;&gt;2.2 Data Preprocessing&lt;/h2&gt;
&lt;h3 id=&#34;221-reading-the-dataset&#34;&gt;2.2.1 Reading the Dataset&lt;/h3&gt;
&lt;h3 id=&#34;222-data-preparation&#34;&gt;2.2.2 Data Preparation&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;inputs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_dummies&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dummy_na&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;inputs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;inputs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inputs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;223-conversion-to-the-tensor-format&#34;&gt;2.2.3 Conversion To the Tensor Format&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;jax&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inputs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_numpy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;float&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;targets&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_numpy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;float&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;23-linear-algebra&#34;&gt;2.3 Linear Algebra&lt;/h2&gt;
&lt;h3 id=&#34;231-scalars&#34;&gt;2.3.1 Scalars&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;x = jnp.array(3.0)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;y = jnp.array(2.0)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;232-vectors&#34;&gt;2.3.2 Vectors&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;x = jnp.arange(3)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;x
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;x[2]
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;len(x)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;x.shape
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;233-matrices&#34;&gt;2.3.3 Matrices&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arange&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;234-tensors&#34;&gt;2.3.4 Tensors&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arange&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;24&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;235-basic-properties-of-tensor-arithmetic&#34;&gt;2.3.5 Basic Properties of Tensor Arithmetic&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;element-wise addition&lt;/li&gt;
&lt;li&gt;element-wise multiplication (Hadamard Product)&lt;/li&gt;
&lt;li&gt;element * scalar&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;236-reduction&#34;&gt;2.3.6 Reduction&lt;/h3&gt;
&lt;p&gt;By default, invoking the sum function reduces a tensor along all of its axes, eventually producing a scalar. Our libraries also allow us to [specify the axes along which the tensor should be reduced.] To sum over all elements along the rows (axis 0), we specify axis=0 in sum. Since the input matrix reduces along axis 0 to generate the output vector, this axis is missing from the shape of the output.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;C&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arange&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;24&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;237-non-reduction-sum&#34;&gt;2.3.7 Non-Reduction Sum&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;print(A)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sum_A = A.sum(axis=1, keepdims=True)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sum_A, sum_A.shape
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;[[0. 1. 2.]
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; [3. 4. 5.]]
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;(Array([[ 3.],
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        [12.]], dtype=float32),
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; (2, 1))
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;A.cumsum(axis=0)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;238-dot-products&#34;&gt;2.3.8 Dot Products&lt;/h3&gt;
&lt;p&gt;weighted sum/ weighted average&amp;hellip;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ones&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;float32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;239-matrix-vector-products&#34;&gt;2.3.9 Matrix-Vector Products&lt;/h3&gt;
&lt;h3 id=&#34;2310-matrix-matrix-products&#34;&gt;2.3.10 Matrix-Matrix Products&lt;/h3&gt;
&lt;p&gt;n&lt;em&gt;k X k&lt;/em&gt;m
-&amp;gt; n * m dot products&lt;/p&gt;
&lt;h3 id=&#34;2311-norms&#34;&gt;2.3.11 Norms&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;u = jnp.array([3.0, -4.0])
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;jnp.linalg.norm(u)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;jnp.linalg.norm(u, ord=1) # same as jnp.abs(u).sum()
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;jnp.linalg.norm(jnp.ones((4, 9)))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;2312-discussions&#34;&gt;2.3.12 Discussions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Scalars, vectors, matrices, and tensors are the basic mathematical objects used in linear algebra and have zero, one, two, and an arbitrary number of axes, respectively.&lt;/li&gt;
&lt;li&gt;Tensors can be sliced or reduced along specified axes via indexing, or operations such as sum and mean, respectively.&lt;/li&gt;
&lt;li&gt;Elementwise products are called Hadamard products. By contrast, dot products, matrix–vector products, and matrix–matrix products are not elementwise operations and in general return objects having shapes that are different from the the operands.&lt;/li&gt;
&lt;li&gt;Compared to Hadamard products, matrix–matrix products take considerably longer to compute (cubic rather than quadratic time).&lt;/li&gt;
&lt;li&gt;Norms capture various notions of the magnitude of a vector (or matrix), and are commonly applied to the difference of two vectors to measure their distance apart.&lt;/li&gt;
&lt;li&gt;Common vector norms include the l1 and l2 norms, and common matrix norms include the spectral and Frobenius norms.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>D2L - Chapter 2.1 Data Manipulation</title>
      <link>http://localhost:1313/post/2025-04-07-d2l-chapter2-1-data-manipulation/</link>
      <pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2025-04-07-d2l-chapter2-1-data-manipulation/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/d2l-ai/d2l-jax-colab/blob/master/chapter_preliminaries/ndarray.ipynb&#34;&gt;colab&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;211-get-started&#34;&gt;2.1.1 Get Started&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;jax&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;jax&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arange&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mi&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;11&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;int32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ones&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Any call of a random function in JAX requires a key to be&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# specified, feeding the same key to a random function will&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# always result in the same sample being generated&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;jax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;normal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;jax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;PRNGKey&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;personal-note&#34;&gt;Personal Note&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;shape -&amp;gt; (axis-0, axis-1, &amp;hellip;)&lt;/li&gt;
&lt;li&gt;reshape must be multiple&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;212-indexing-and-slicing&#34;&gt;2.1.2 Indexing and Slicing&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# JAX arrays are immutable. jax.numpy.ndarray.at index&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# update operators create a new array with the corresponding&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# modifications made&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_new_1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;at&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;17&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_new_1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_new_2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X_new_1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;at&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_new_2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;213-operations&#34;&gt;2.1.3 Operations&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arange&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;float32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;2.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# axis sets which direction to stack, axis = 0, stacks on the row, meaning will have 6 in axis-0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;concatenate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;concatenate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;personal-note-1&#34;&gt;Personal Note&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;axis sets which direction to stack. For example, axis = 0, stacks on the row, meaning will have 6(3 + 3) items in axis-0.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;214-broadcasting&#34;&gt;2.1.4 Broadcasting&lt;/h3&gt;
&lt;p&gt;By now, you know how to perform elementwise binary operations on two tensors of the same shape. Under certain conditions, even when shapes differ, we can still [perform elementwise binary operations by invoking the broadcasting mechanism.] Broadcasting works according to the following two-step procedure: (i) expand one or both arrays by copying elements along axes with length 1 so that after this transformation, the two tensors have the same shape; (ii) perform an elementwise operation on the resulting arrays.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arange&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arange&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;personal-note-2&#34;&gt;Personal Note&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;broadcasting copies data to reach lcm and do computation.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;215-conversion-to-other-python-objects&#34;&gt;2.1.5 Conversion to Other Python Objects&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jnp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;3.5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;item&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;float&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;🙅&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;🙅&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>D2L - Introduction - 2/2</title>
      <link>http://localhost:1313/post/2025-04-07-d2l-chapter1-2-introduction-2-complete/</link>
      <pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2025-04-07-d2l-chapter1-2-introduction-2-complete/</guid>
      <description>&lt;h2 id=&#34;132-unsupervised-and-self-supervised-learning&#34;&gt;1.3.2 UnSupervised and Self-Supervised Learning&lt;/h2&gt;
&lt;p&gt;The author lists the difference between traditional software engineering and machine learning. While software engineering focuses on solving problems with rule-based solutions, machine learning(deep learning) tries to solve problems that are ambiguous and the solution can evlove by itself after seeing more examples.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Clustering&lt;/li&gt;
&lt;li&gt;PCA&lt;/li&gt;
&lt;li&gt;Relationship, Rome - Italy = France - Paris&lt;/li&gt;
&lt;li&gt;causality and probabilistic graphical models&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;133-interacting-with-an-environment&#34;&gt;1.3.3 Interacting with an Environment&lt;/h3&gt;
&lt;h3 id=&#34;134-reinforcement-learning&#34;&gt;1.3.4 Reinforcement Learning&lt;/h3&gt;
&lt;h2 id=&#34;14-roots&#34;&gt;1.4 Roots&lt;/h2&gt;
&lt;h2 id=&#34;15-the-road-to-deep-learning&#34;&gt;1.5 The road to deep learning&lt;/h2&gt;
&lt;h2 id=&#34;16-success-stories&#34;&gt;1.6 Success Stories&lt;/h2&gt;
&lt;h2 id=&#34;17-the-essence-of-deep-learning&#34;&gt;1.7 The essence of deep learning&lt;/h2&gt;
&lt;p&gt;Thus, one key advantage of deep learning is that it replaces not only the shallow models at the end of traditional learning pipelines, but also the labor-intensive process of feature engineering.&lt;/p&gt;
&lt;h2 id=&#34;18-summary&#34;&gt;1.8 Summary&lt;/h2&gt;
&lt;h2 id=&#34;my-takeaway&#34;&gt;My takeaway&lt;/h2&gt;
&lt;p&gt;Chapter1. Introduction mainly discussed machine learning from data, definition, examples, history, current success, provided foundation to future studies.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>D2L - Introduction</title>
      <link>http://localhost:1313/post/2025-04-02-d2l-chapter1-1-introduction-1/</link>
      <pubDate>Wed, 02 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2025-04-02-d2l-chapter1-1-introduction-1/</guid>
      <description>&lt;h2 id=&#34;10-introduction&#34;&gt;1.0 Introduction&lt;/h2&gt;
&lt;p&gt;The author lists the difference between traditional software engineering and machine learning. While software engineering focuses on solving problems with rule-based solutions, machine learning(deep learning) tries to solve problems that are ambiguous and the solution can evlove by itself after seeing more examples.&lt;/p&gt;
&lt;h2 id=&#34;11-a-motivating-example&#34;&gt;1.1 A Motivating Example&lt;/h2&gt;
&lt;p&gt;The author lists an example of a wake word system where we feed model with data and let the model learn.&lt;/p&gt;
&lt;h2 id=&#34;12-key-components&#34;&gt;1.2 Key Components&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The data that we can learn from.&lt;/li&gt;
&lt;li&gt;A model of how to transform the data.&lt;/li&gt;
&lt;li&gt;An objective function that quantifies how well or badly the model is doing.&lt;/li&gt;
&lt;li&gt;An algorithm to adjust the model&amp;rsquo;s parameters to optimize the objective function.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;121-data&#34;&gt;1.2.1 Data&lt;/h3&gt;
&lt;p&gt;Machine learning relies on data, which consists of examples represented numerically through features (inputs) and, in supervised learning, labels (targets). Examples include images or medical records, which can be structured as fixed-length vectors—but not all data fits this format, especially text or variable-sized inputs. Deep learning models handle such complexity more effectively than traditional methods. However, more data generally leads to better models, especially in deep learning, where large datasets are often essential. Crucially, the quality and representativeness of the data matter just as much as quantity—biased or flawed data can lead to harmful and unfair outcomes, even without malicious intent.&lt;/p&gt;
&lt;h3 id=&#34;122-models&#34;&gt;1.2.2 Models&lt;/h3&gt;
&lt;p&gt;Most machine learning involves transforming input data to make predictions. A model refers to the system that performs this transformation—taking data of one type and outputting another, such as predicting emotions from photos or detecting anomalies in sensor readings. While simple models can solve basic problems, more complex challenges often require more powerful tools. Deep learning stands out by using layered models that apply many transformations in sequence, hence the term deep. This approach enables tackling problems that push beyond the limits of classical methods, though traditional techniques still play a role along the way.&lt;/p&gt;
&lt;h3 id=&#34;123-objective-function&#34;&gt;1.2.3 Objective function&lt;/h3&gt;
&lt;p&gt;In machine learning, learning means improving performance at a task over time, which requires a clear way to measure improvement. These measurements are called objective functions or loss functions, which we usually define so that lower values are better. Common examples include squared error for regression and error rate for classification. Some losses are hard to optimize directly, so we often use surrogate objectives that are easier to work with.&lt;/p&gt;
&lt;p&gt;We train models by minimizing loss on a training dataset, treating the data as fixed and adjusting model parameters. However, strong performance on training data doesn&amp;rsquo;t guarantee success on new, unseen data. To evaluate generalization, we use a separate test dataset. If a model performs well on training data but poorly on test data, it&amp;rsquo;s said to be overfitting—essentially memorizing rather than learning.&lt;/p&gt;
&lt;h3 id=&#34;124-optimization-algorithms&#34;&gt;1.2.4 Optimization Algorithms&lt;/h3&gt;
&lt;p&gt;ChatGPT said:
Once we have data, a model, and an objective function, the next step is to optimize the model’s parameters to minimize the loss. In deep learning, this is typically done using gradient descent. This algorithm works by computing how a small change in each parameter would affect the loss, then adjusting the parameters in the direction that reduces the loss. Repeating this process iteratively helps the model gradually improve its performance.&lt;/p&gt;
&lt;h2 id=&#34;13-kinds-of-machine-learning-problems&#34;&gt;1.3 Kinds of Machine Learning Problems&lt;/h2&gt;
&lt;h3 id=&#34;131-supervised-learning&#34;&gt;1.3.1 Supervised Learning&lt;/h3&gt;
&lt;h3 id=&#34;1311-regression&#34;&gt;1.3.1.1 Regression&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Question: How many?&lt;/li&gt;
&lt;li&gt;Loss: squared error&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;1312-classification&#34;&gt;1.3.1.2 Classification&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Question: Which one?&lt;/li&gt;
&lt;li&gt;Loss: cross-entropy&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;1313-tagging&#34;&gt;1.3.1.3 Tagging&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Question: multi-label classification&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;1314-search&#34;&gt;1.3.1.4 Search&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Question: releance scores&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;1315-recommender-system&#34;&gt;1.3.1.5 Recommender System&lt;/h3&gt;
&lt;h3 id=&#34;1316-sequence-learning&#34;&gt;1.3.1.6 Sequence Learning&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;They require a model either to ingest sequences of inputs or to emit sequences of outputs (or both).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cases&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Tagging and Parsing: tag named entity&lt;/li&gt;
&lt;li&gt;Automatic Speech Recognition: challenge is that input is way longer than output&lt;/li&gt;
&lt;li&gt;Text to Speech: This is the inverse of asr.&lt;/li&gt;
&lt;li&gt;Machine Translation: input/output might not align in the same order.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>CS224n-NLP-lecture 1, 2</title>
      <link>http://localhost:1313/post/2024-11-29-cs224n-nlp-lecture1&#43;2-word2vec/</link>
      <pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2024-11-29-cs224n-nlp-lecture1&#43;2-word2vec/</guid>
      <description>&lt;h1 id=&#34;word2vector&#34;&gt;Word2Vector&lt;/h1&gt;
&lt;h2 id=&#34;how&#34;&gt;How&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Represent words by their context (Distributional semantics): A word&amp;rsquo;s meaning is given by words frequently appear close-by, within a fixed-size.&lt;/li&gt;
&lt;li&gt;Use the many contexts of w to build up a representation of w.&lt;/li&gt;
&lt;li&gt;We build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar context.&lt;/li&gt;
&lt;li&gt;Note that word vectors are also called word embeddings, or neural word representations. They are a distributed representation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;word2vector-idea&#34;&gt;Word2Vector idea&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We have a large body of text (corpus).&lt;/li&gt;
&lt;li&gt;Every word in a fixed vocabulary is represented by vector.&lt;/li&gt;
&lt;li&gt;Go through each position t in the text, which has a center word (c) and a context word (o).&lt;/li&gt;
&lt;li&gt;Use similarity of word vectors for c and o, calculate the probablity of o given c.&lt;/li&gt;
&lt;li&gt;Keep adjusting the word vector to maximize this probablity.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;$L(\theta) = \prod_{t=1}^T \prod_{\substack{-m \leq j \leq m \\ j \neq 0}} P(w_{t+j} \mid w_t; \theta)$&lt;/li&gt;
&lt;li&gt;$J(\theta) = -\frac{1}{T} \log L(\theta) = -\frac{1}{T} \sum_{t=1}^T \sum_{\substack{-m \leq j \leq m \\ j \neq 0}} \log P(w_{t+j} \mid w_t; \theta)$&lt;/li&gt;
&lt;li&gt;$P(o \mid c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V} \exp(u_w^T v_c)}$
&lt;ul&gt;
&lt;li&gt;Exponentiation makes anything positive&lt;/li&gt;
&lt;li&gt;Dot product compares similarity of o and c.&lt;/li&gt;
&lt;li&gt;Normalize over entire vocabulary to give probability distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;gradient-descent-and-stochastic-gradient-descent&#34;&gt;Gradient Descent and Stochastic Gradient Descent&lt;/h2&gt;
&lt;h3 id=&#34;problem&#34;&gt;Problem:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;J is a function of all windows in the corpus, gradient is expensive to compute.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solution&#34;&gt;Solution:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SGD Stochastic Gradient Descent&lt;/li&gt;
&lt;li&gt;SGD in each window: we only have at most 2m + 1 words&lt;/li&gt;
&lt;li&gt;Either we need sparse matrix updates operations to only update certain rows of full embedding metrics U and V, or we need to keep around a hash of word vector. Millions of word vectors allow us to do distributed computing.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;word2vector-algo-family&#34;&gt;Word2Vector Algo Family&lt;/h2&gt;
&lt;h3 id=&#34;two-models&#34;&gt;Two models:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Skip-gram(SG), predict context (outside) words given center words.&lt;/li&gt;
&lt;li&gt;Continuous bag of words (CBOW), predict context words from context words.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;negative-sampling&#34;&gt;Negative Sampling&lt;/h3&gt;
&lt;h4 id=&#34;problem-1&#34;&gt;Problem&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The normalization demonitator is computionally expensive.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;solution-1&#34;&gt;Solution&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Train a binary logistic regression for a true pair vs noise pair.&lt;/li&gt;
&lt;li&gt;$J(\theta) = \frac{1}{T} \sum_{t=1}^{T} J_t(\theta)$&lt;/li&gt;
&lt;li&gt;$J_t(\theta) = \log \sigma \left( u_o^\top v_c \right) + \sum_{i=1}^{k} \mathbb{E}_{j \sim P(w)} \left[ \log \sigma \left( -u_j^\top v_c \right) \right]$&lt;/li&gt;
&lt;li&gt;$\sigma(x) = \frac{1}{1 + e^{-x}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Take k negative samples&lt;/li&gt;
&lt;li&gt;Maxmize probablity that real words appear, minimize probablity that random words appear around the center words.&lt;/li&gt;
&lt;li&gt;Sample with P(w) = u(w)^(3/4)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;co-occurence-counting&#34;&gt;Co-occurence counting&lt;/h2&gt;
&lt;h3 id=&#34;problems&#34;&gt;Problems:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;vector increase in sizes&lt;/li&gt;
&lt;li&gt;Subsequent classification models have sparsity issues.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;singular-value-decomposition&#34;&gt;Singular Value Decomposition&lt;/h3&gt;
&lt;p&gt;X = USV&lt;/p&gt;
&lt;h3 id=&#34;glove&#34;&gt;GloVe&lt;/h3&gt;
&lt;p&gt;Crucial Insight: Ratios of co-occurence probablities can encode meaning components (meaning component means going from male to female etc).&lt;/p&gt;
&lt;p&gt;$J = \sum_{i,j=1}^{V} f(X_{ij}) \left( w_i^\top \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij} \right)^2$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$X\_{ij}$: The co-occurrence count between words \( i \) and \( j \).&lt;/li&gt;
&lt;li&gt;\( w_i \): The word vector for the word \( i \).&lt;/li&gt;
&lt;li&gt;\( \tilde{w}\_j \): The context word vector for the word \( j \).&lt;/li&gt;
&lt;li&gt;\( b_i \): The bias term for the word \( i \).&lt;/li&gt;
&lt;li&gt;\( \tilde{b}\_j \): The bias term for the context word \( j \).&lt;/li&gt;
&lt;li&gt;\( \log X*{ij} \): The logarithm of the co-occurrence count \( X*{ij} \).&lt;/li&gt;
&lt;li&gt;\( f(X\_{ij}) \): A weighting function to limit the influence of very frequent or very infrequent word pairs.&lt;/li&gt;
&lt;li&gt;\( x\_{\text{max}} \): A maximum co-occurrence count threshold.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-to-evaluate&#34;&gt;How to evaluate&lt;/h2&gt;
&lt;h3 id=&#34;intrinsics&#34;&gt;Intrinsics&lt;/h3&gt;
&lt;h4 id=&#34;definition&#34;&gt;Definition&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Evaluation on a specific subtask.&lt;/li&gt;
&lt;li&gt;Fast to compute.&lt;/li&gt;
&lt;li&gt;Helps to understand the system.&lt;/li&gt;
&lt;li&gt;Not clear if it really helps unless correlation to real task is established.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;example&#34;&gt;Example&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Word Vector Analogy&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;man:woman :: king:?&lt;/li&gt;
&lt;li&gt;Evaluate word vectors by how will their cosine distance after addition captures intuitive semantics and syntatic analogy questions.&lt;/li&gt;
&lt;li&gt;Discard the input words from search&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;WordSim human judgement&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;extrinsics&#34;&gt;Extrinsics&lt;/h3&gt;
&lt;h4 id=&#34;definition-1&#34;&gt;Definition&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Evaluation on a real task.&lt;/li&gt;
&lt;li&gt;Can take a long time to compute.&lt;/li&gt;
&lt;li&gt;Unclear if the subsystem is the probelm or its interaction or other subsystems.&lt;/li&gt;
&lt;li&gt;If replacing exactly one subsystem is making progress, that&amp;rsquo;s good.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;example-1&#34;&gt;Example&lt;/h4&gt;
&lt;p&gt;Named Entity recognition.&lt;/p&gt;
&lt;h2 id=&#34;word-sense&#34;&gt;Word sense&lt;/h2&gt;
&lt;p&gt;A word can have multiple meanings.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Recommender System Study Notes - Nomination(6/6)</title>
      <link>http://localhost:1313/post/2024-09-06-rec-sys-nomination-6/</link>
      <pubDate>Fri, 06 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2024-09-06-rec-sys-nomination-6/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;GeoHash Nomination&lt;/li&gt;
&lt;li&gt;Author Nomination
&lt;ul&gt;
&lt;li&gt;Subscribed Author&lt;/li&gt;
&lt;li&gt;Interacted Author&lt;/li&gt;
&lt;li&gt;Similar Author&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cache Nomination&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;geohash-nomination&#34;&gt;GeoHash Nomination&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Indexing
&lt;ul&gt;
&lt;li&gt;GeoHash -&amp;gt; Item with good quality(reverse chronological order)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;No personalized&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;subscribed-author-nomination&#34;&gt;Subscribed Author Nomination&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Indexing:
&lt;ul&gt;
&lt;li&gt;User -&amp;gt; subscribed authors&lt;/li&gt;
&lt;li&gt;author -&amp;gt; uploaded items&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Nomination:
&lt;ul&gt;
&lt;li&gt;User -&amp;gt; Subscribed Author -&amp;gt; New Item&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;interacted-author-nomination&#34;&gt;Interacted Author Nomination&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If someone likes/love/forward an item&lt;/li&gt;
&lt;li&gt;Indexing:
&lt;ul&gt;
&lt;li&gt;User -&amp;gt; Interacted Author (update it)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Nomination:
&lt;ul&gt;
&lt;li&gt;User -&amp;gt; Interacted Author -&amp;gt; New Item&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;similar-author-nomination&#34;&gt;Similar Author Nomination&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Indexing:
&lt;ul&gt;
&lt;li&gt;Author -&amp;gt; Similar Author&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Nomination:
&lt;ul&gt;
&lt;li&gt;User ID -&amp;gt; Liked Author -&amp;gt; Similar Author -&amp;gt; New Item&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cache-nomination&#34;&gt;Cache Nomination&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Intuition: Use the previous ranking winning candidates discarded by re-ranking.&lt;/li&gt;
&lt;li&gt;Top 50 items, but get discarded by re-ranking, go to nomination.&lt;/li&gt;
&lt;li&gt;Cache is fixed. How to clear the cache
&lt;ul&gt;
&lt;li&gt;Got impressed&lt;/li&gt;
&lt;li&gt;At most 10 times&lt;/li&gt;
&lt;li&gt;At most 3 days&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bloom-filter&#34;&gt;Bloom Filter&lt;/h3&gt;
&lt;h4 id=&#34;intuition&#34;&gt;Intuition&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;If user has seen an item, then we do not recommend this item to user&lt;/li&gt;
&lt;li&gt;For each user, record item that has been recommended(maybe for a month)&lt;/li&gt;
&lt;li&gt;For each item, check if it&amp;rsquo;s been recommended before&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;basics&#34;&gt;Basics&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The Bloom filter determines whether an item ID is in the set of exposed items.&lt;/li&gt;
&lt;li&gt;If the judgment is &amp;ldquo;no&amp;rdquo;, then the item is definitely not in the set.&lt;/li&gt;
&lt;li&gt;If the judgment is &amp;ldquo;yes&amp;rdquo;, then the item is likely in the set. (There may be false positives, where an unexposed item is incorrectly judged as exposed and thus filtered out.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;heading&#34;&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The size of the exposed item set is \( n \), the dimensionality of the binary vector is \( m \), and \( k \) hash functions are used.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The false positive probability of the Bloom filter \( \delta \) is approximately:&lt;/p&gt;
\[
  \delta \approx \left( 1 - \exp\left( \frac{-kn}{m} \right) \right)^k
  \]
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As \( n \) increases, the more 1&amp;rsquo;s there are in the vector, and the higher the false positive probability. (The probability that all \( k \) positions corresponding to unexposed items are set to 1 increases.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As \( m \) increases, the vector becomes longer, making hash collisions less likely.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If \( k \) is too large or too small, it is not ideal; \( k \) has an optimal value.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Exposed Filtering Path:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;               +-------------+              +---------+              +-------------+
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;               |   Recall 1  |------------&amp;gt; | Sorting |------------&amp;gt; |   Item 1    |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;               +-------------+              +---------+              +-------------+
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;               |   Recall 2  |                                                   |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;               +-------------+                                                   |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;               |   Recall 3  |                                                   |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;               +-------------+                                                   v
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                                                           +-------------+
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                                                           |   Item 2    |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                                                           +-------------+
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                                                           |     ...     |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                                                           +-------------+
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                                                           |   Item q    |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                                                           +-------------+
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                      ^                                                        |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                      |                                                        |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;+------------------+  |                                                        |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;| Binary Vector    |&amp;lt;-+                                                        |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;+------------------+                                                           v
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                             +-------------------+     +-----------------------+
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                             | Bloom Filter      |&amp;lt;----| Real-time Processing   |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                             | (Exposure Filter) |     |  (Kafka + Flink)       |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                             +-------------------+     +-----------------------+
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Recommender System Study Notes - Nomination(5/6)</title>
      <link>http://localhost:1313/post/2024-09-05-rec-sys-nomination-5/</link>
      <pubDate>Thu, 05 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2024-09-05-rec-sys-nomination-5/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Recommender System Study Notes - Nomination(4/6)</title>
      <link>http://localhost:1313/post/2024-09-04-rec-sys-nomination-4/</link>
      <pubDate>Wed, 04 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2024-09-04-rec-sys-nomination-4/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Two tower model full update/incremental update.
&lt;ul&gt;
&lt;li&gt;Online Nomination: Online compute user ID, ANN in stored vector database with item.&lt;/li&gt;
&lt;li&gt;Full Update: 1 epoch, 1 day.&lt;/li&gt;
&lt;li&gt;Incremental Update: Only upadte ID embedding, frozen the fully connected layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two tower model Self-supervised learning
&lt;ul&gt;
&lt;li&gt;Problem: Two tower cannot learn item with limited amount of clicks (data biased).&lt;/li&gt;
&lt;li&gt;Self-supervised learning: random feature transformation&lt;/li&gt;
&lt;li&gt;bi&amp;rsquo;, bi&amp;rsquo;&amp;rsquo; high similarity&lt;/li&gt;
&lt;li&gt;bi&amp;rsquo;, bj&amp;rsquo;&amp;rsquo; low similarity&lt;/li&gt;
&lt;li&gt;Training:
&lt;ul&gt;
&lt;li&gt;pick n (user - item) as a batch for training.&lt;/li&gt;
&lt;li&gt;m items as a batch(uniform sampling) for self-supervised learning.&lt;/li&gt;
&lt;li&gt;$\frac{1}{n} \sum_{i=1}^{n} L_{\text{main}}[i] + \alpha \cdot \frac{1}{m} \sum_{j=1}^{m} L_{\text{self}}[j]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;online-nomination&#34;&gt;Online Nomination&lt;/h3&gt;
&lt;h4 id=&#34;implementation&#34;&gt;Implementation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Item
&lt;ul&gt;
&lt;li&gt;Use neural network to compute item b&amp;rsquo;s embedding.&lt;/li&gt;
&lt;li&gt;Store them in vectorized databse (Milvus, Faiss, HnswLib).&lt;/li&gt;
&lt;li&gt;Indexing, for ANN search.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;User
&lt;ul&gt;
&lt;li&gt;Use user ID, feature, etc to compute user a&amp;rsquo;s embedding.&lt;/li&gt;
&lt;li&gt;ANN search
&lt;ul&gt;
&lt;li&gt;Use a as query.&lt;/li&gt;
&lt;li&gt;Return the top k as return.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Why store item but compute user online?
&lt;ul&gt;
&lt;li&gt;User is only 1, item is O(b).&lt;/li&gt;
&lt;li&gt;User often changes quickly.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model-update&#34;&gt;Model Update&lt;/h3&gt;
&lt;h4 id=&#34;full-update-vs-incremental-update&#34;&gt;Full Update VS Incremental Update&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Full Update
&lt;ul&gt;
&lt;li&gt;Use yesterday&amp;rsquo;s tfrecord, train 1 epoch.&lt;/li&gt;
&lt;li&gt;Update the new user neural network, store item data into database.&lt;/li&gt;
&lt;li&gt;Easy to implement, easy requirement for the system.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Incremental Update(online learning)
&lt;ul&gt;
&lt;li&gt;Why? User&amp;rsquo;s interest can change quickly.&lt;/li&gt;
&lt;li&gt;Data streaming, TFRecord.&lt;/li&gt;
&lt;li&gt;Online learning, gradient descent, only update ID Embedding, not other parameters in the neural network.&lt;/li&gt;
&lt;li&gt;Update the new ID embedding.&lt;/li&gt;
&lt;li&gt;To be discarded after full update.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Why only incremental update is not having the best result?
&lt;ul&gt;
&lt;li&gt;Hourly data is biased.&lt;/li&gt;
&lt;li&gt;Full update: random shuffle, 1 epoch.&lt;/li&gt;
&lt;li&gt;Incremental update: train from morning to night, this is not having the best result.&lt;/li&gt;
&lt;li&gt;random shuffle makes a positive impact on the learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;two-tower-model-self-supervised-learning&#34;&gt;Two tower model Self-supervised learning&lt;/h3&gt;
&lt;h4 id=&#34;problem&#34;&gt;Problem&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The head effect in the recommendation system is severe:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A small number of items account for most of the clicks.&lt;/li&gt;
&lt;li&gt;Most items have a low number of clicks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The features of high-click items are well-learned, but the features of long-tail items are not well-learned.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Self-supervised learning: data augumentation, to better learn long-tail items&amp;rsquo; embedding.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;intuition&#34;&gt;Intuition&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A simple feature transformation will make minimal impact on their feature embeddings, and pairwise, the similarity still remains large.&lt;/li&gt;
&lt;li&gt;i -&amp;gt; i&amp;rsquo; -&amp;gt; bi&amp;rsquo;, i -&amp;gt; i&amp;rsquo;&amp;rsquo; -&amp;gt; bi&amp;rsquo;&amp;rsquo;, j -&amp;gt; j&amp;rsquo; -&amp;gt; bj&amp;rsquo;, j -&amp;gt; j&amp;rsquo;&amp;rsquo;-&amp;gt;bj&#39;&#39;&lt;/li&gt;
&lt;li&gt;i, bi&amp;rsquo; and bi&amp;rsquo;&amp;rsquo; has high similarity&lt;/li&gt;
&lt;li&gt;i, j, bi&amp;rsquo; and bj&amp;rsquo;&amp;rsquo; has low similarity&lt;/li&gt;
&lt;li&gt;cos(bi, bi&amp;rsquo;&amp;rsquo;) big, cos(bi&amp;rsquo;, bj&amp;rsquo;) small&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;methods&#34;&gt;Methods&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Random Mask&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Randomly pick labels, and set them to null.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Dropout&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Randomly dropout 50% label.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Complementary&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;ID, Label, Keywords, City.&lt;/li&gt;
&lt;li&gt;Randomly divide them into two {ID, keyword} and {Label, City}.&lt;/li&gt;
&lt;li&gt;{ID, default, Keyword, default} -&amp;gt; Item embedding.&lt;/li&gt;
&lt;li&gt;{default, Label, default, City} -&amp;gt; Item embedding.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Mask a set of correlated features&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;U = {Male, Female, Unisex}.&lt;/li&gt;
&lt;li&gt;V = {Photography, Football, IT, Makeup}.&lt;/li&gt;
&lt;li&gt;u = female, v = Makeup, p(u, v) is high.&lt;/li&gt;
&lt;li&gt;u = female, v = IT, p(u, v) is low.&lt;/li&gt;
&lt;li&gt;$MI(U, V) = \sum_{u \in U} \sum_{v \in V} p(u, v) \cdot \log \frac{p(u, v)}{p(u) \cdot p(v)}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Learning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assume there are a total of \( k \) features. Calculate the MI (Mutual Information) between any two features offline, resulting in a \( k \times k \) matrix.&lt;/li&gt;
&lt;li&gt;Randomly select one feature as a seed and find the \( k/2 \) features most related to the seed.&lt;/li&gt;
&lt;li&gt;Mask the seed and its related \( k/2 \) features, retaining the remaining \( k/2 \) features.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Training:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sampling m items as a batch&lt;/li&gt;
&lt;li&gt;cos(bi&amp;rsquo;, bi&amp;rsquo;&amp;rsquo;) -&amp;gt; 1, other cos(bi&amp;rsquo;, bj&amp;rsquo;&amp;rsquo;) -&amp;gt; 0&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Recommender System Study Notes - Nomination(3/6)</title>
      <link>http://localhost:1313/post/2024-09-03-rec-sys-nomination-3/</link>
      <pubDate>Tue, 03 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2024-09-03-rec-sys-nomination-3/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Two Tower Model - Architecture + Training&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pointwise Training
&lt;ul&gt;
&lt;li&gt;View nomination as binary classification&lt;/li&gt;
&lt;li&gt;Sample: cos(a,b) -&amp;gt; +1, negative sample: cos(a, b) -&amp;gt; -1&lt;/li&gt;
&lt;li&gt;Positive : negative = 1:2 / 1:3&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pairwise Training
&lt;ul&gt;
&lt;li&gt;Intuntion: cos(a, b+) &amp;gt; cos(a, b-)&lt;/li&gt;
&lt;li&gt;Triplet hinge loss = max(0, cos(a, b-) + m - cos(a, b+))&lt;/li&gt;
&lt;li&gt;Triplet logistic loss = log(1 + exp(σ) * cos(a, b-) - cos(a, b+))&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Listwise Training
&lt;ul&gt;
&lt;li&gt;CrossEntropyLoss&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Two Tower Model - Positive Samples + Negative Samples&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Positive Sample:
&lt;ul&gt;
&lt;li&gt;Impressed and clicked&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Negative Sample:&lt;/li&gt;
&lt;li&gt;Easy negative sample:
&lt;ul&gt;
&lt;li&gt;All items&lt;/li&gt;
&lt;li&gt;In-batch items&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Hard negative sample:
&lt;ul&gt;
&lt;li&gt;Nominated, but discarded by ranking&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Mix easy and hard for training dataset&lt;/li&gt;
&lt;li&gt;Wrong sampling:
&lt;ul&gt;
&lt;li&gt;Impressed, but not clicked (can be used for training ranking but not nomination)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;two-tower-modeldssm---architecture-and-training&#34;&gt;Two Tower Model(DSSM) - Architecture and Training&lt;/h3&gt;
&lt;h4 id=&#34;two-tower-model-architecture&#34;&gt;Two Tower Model Architecture&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Neural Network
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    └── Concatenate
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        ├── Embedding Layer (User ID)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        │      └── Embedding (Vector Representation)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        ├── Embedding Layers (User Discrete Feature)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        │      └── Embedding (Vector Representation)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        └── Normalization, Binning, etc. (normalize to 1, put into buckets)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;               └── User Continuous Features (Vector Representation)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;$\cos(a, b) = \frac{a \cdot b}{\|a\|_2 \cdot \|b\|_2}$&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                            Cosine Similarity:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                  /    \
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                /        \
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                             a            b
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                          /                 \
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;              Neural Network              Neural Network
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                /                                \
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;         Feature Transformation           Feature Transformation
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;         /                                          \
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;User ID, Discrete Features,                 Product ID, Discrete Features,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Continuous Features                         Continuous Features
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;two-tower-model-training&#34;&gt;Two Tower Model Training&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Pointwise (binary classification)&lt;/li&gt;
&lt;li&gt;Pairwise&lt;/li&gt;
&lt;li&gt;Listwise&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;pointwise-training&#34;&gt;Pointwise Training&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Intuition: View nomination as binary classification&lt;/li&gt;
&lt;li&gt;positive sample: cos(a,b) -&amp;gt; +1, negative sample: cos(a, b) -&amp;gt; -1&lt;/li&gt;
&lt;li&gt;positive : negative = 1:2 / 1:3&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;pairwise-training&#34;&gt;Pairwise Training&lt;/h5&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                    cos(a, b⁺)          cos(a, b⁻)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                    /      \            /      \
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                   /        \          /        \
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                b⁺               a                  b⁻
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        (Positive Sample)       (User)           (Negative Sample)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                |                  |                    |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        Neural Network      Neural Network     Neural Network
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                |                  |                    |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Feature Transformation   Feature Transformation Feature Transformation
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                |                  |                    |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Positive Item Feature     User Features          Negative Item Feature
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Intuntion: cos(a, b+) &amp;gt; cos(a, b-)&lt;/li&gt;
&lt;li&gt;triplet hinge loss = max(0, cos(a, b-) + m - cos(a, b+))&lt;/li&gt;
&lt;li&gt;triplet logistic loss = log(1 + exp(σ) * cos(a, b-) - cos(a, b+))&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;listwise-training&#34;&gt;Listwise Training&lt;/h5&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;CrossEntropyLoss(y, s) = -log(s⁺)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                       +-----------------------+
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                       |    CrossEntropyLoss   |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                       +-----------------------+
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                 |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                 |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                          +-------------------------------+
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                          |  Softmax Activation Function  |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                          +-------------------------------+
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                 |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      +------------+------------+-------------+ ... +------------+
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      |            |            |             |     |            |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     s⁺         s⁻_1       s⁻_2         ...   s⁻_n
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; (Positive)    (Negative)  (Negative)        (Negative)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    |             |             |                     |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    |             |             |                     |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; cos(a, b⁺)   cos(a, b⁻_1)  cos(a, b⁻_2)   ...  cos(a, b⁻_n)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   |              |             |                     |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;(Positive)    (Negative)   (Negative)         (Negative)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; Sample         Sample        Sample            Sample
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;two-tower-modeldssm---positive-samples--negative-samples&#34;&gt;Two Tower Model(DSSM) - Positive Samples + Negative Samples&lt;/h3&gt;
&lt;h4 id=&#34;positive-samples&#34;&gt;Positive Samples&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;impressed and clicked&lt;/li&gt;
&lt;li&gt;problem: 20% item takes 80% click&lt;/li&gt;
&lt;li&gt;solution:
&lt;ul&gt;
&lt;li&gt;up-sampling: one sample appear multiple times&lt;/li&gt;
&lt;li&gt;down-sampling: discard samples&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;negative-samples&#34;&gt;Negative Samples&lt;/h4&gt;
&lt;h5 id=&#34;simple-negative-sampling-all-items&#34;&gt;Simple Negative Sampling: All Items&lt;/h5&gt;
&lt;h6 id=&#34;uniform-sampling-unfair-to-unpopular-items&#34;&gt;Uniform Sampling: Unfair to unpopular items&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;Most positive samples are popular items.&lt;/li&gt;
&lt;li&gt;If uniform sampling produces negative samples, most of the negative samples are unpopular items.&lt;/li&gt;
&lt;li&gt;positive sample: popular items, negative sample: unpopular items, this is unfair.&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;non-uniform-sampling-the-goal-is-to-suppress-popular-items&#34;&gt;Non-uniform Sampling: The goal is to suppress popular items&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;The probability of negative sampling is positively correlated with popularity (clicks).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sampling probability ∝ (clicks) ^ 0.75&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;simple-negative-sampling-negative-sample-within-a-batch&#34;&gt;Simple Negative Sampling: Negative Sample within a batch&lt;/h5&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;user1 - item1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;user2 - item2
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;user3 - item3
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;user_n - item_n
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;one batch has n positive samples&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;one batch has n(n - 1) negative samples&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The probablity that an item appear in a batch is propotional to the number of times it clicked.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We aim to get it propotional to clicks^0.75.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;offline training cos(a, bi) - log(pi), pi ∝ clicks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;online inference cos(a, bi)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;hard-negative-sampling&#34;&gt;Hard Negative Sampling&lt;/h4&gt;
&lt;h5 id=&#34;hard-negative-samples&#34;&gt;Hard negative samples:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Items that are ranked and close to the target (relatively hard).&lt;/li&gt;
&lt;li&gt;Items that are ranked far behind the target (extremely hard).&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;binary-classification-for-positive-and-negative-samples&#34;&gt;Binary classification for positive and negative samples:&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;All items (simple): High classification accuracy.&lt;/li&gt;
&lt;li&gt;Items that are ranked and close to the target (relatively hard): Easy to misclassify.&lt;/li&gt;
&lt;li&gt;Items that are ranked far behind the target (extremely hard): Even easier to misclassify.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;datasets&#34;&gt;Datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Mix negative samples&lt;/li&gt;
&lt;li&gt;50% easy negative sample&lt;/li&gt;
&lt;li&gt;50% hard negative sample&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Recommender System Study Notes - Nomination(2/6)</title>
      <link>http://localhost:1313/post/2024-09-03-rec-sys-nomination-2/</link>
      <pubDate>Mon, 02 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2024-09-03-rec-sys-nomination-2/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Discrete Feature&lt;/li&gt;
&lt;li&gt;Matrix Completion&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;discrete-feature&#34;&gt;Discrete Feature&lt;/h3&gt;
&lt;h4 id=&#34;example&#34;&gt;Example&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;sex&lt;/li&gt;
&lt;li&gt;nationality&lt;/li&gt;
&lt;li&gt;english words&lt;/li&gt;
&lt;li&gt;item id&lt;/li&gt;
&lt;li&gt;user id&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;how-to-encode-discrete-feature---vectorization&#34;&gt;How to encode discrete feature -&amp;gt; vectorization&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;one-hot encoding&lt;/li&gt;
&lt;li&gt;embedding (*)
&lt;ul&gt;
&lt;li&gt;size: vector dimension * number of items&lt;/li&gt;
&lt;li&gt;How to implement: TensorFlow, PyTorch -&amp;gt; embedding&lt;/li&gt;
&lt;li&gt;embedding = Parameter matrix * one-hot encoding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;matrix-completion&#34;&gt;Matrix Completion&lt;/h3&gt;
&lt;h4 id=&#34;intuition&#34;&gt;Intuition&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;item id -&amp;gt; embedding
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                    \
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                     - cos(a, b)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                    /
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;item id -&amp;gt; embedding
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;formula&#34;&gt;Formula&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;dataset = {u, i, y}&lt;/li&gt;
&lt;li&gt;training, u -&amp;gt; au, i -&amp;gt; bi&lt;/li&gt;
&lt;li&gt;optimization: $\min_{A, B} \sum_{\substack{(u, i, y) \in \Omega}} \left( y - \langle a_u, b_i \rangle \right)^2$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;proscons&#34;&gt;Pros/Cons&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Cons
&lt;ul&gt;
&lt;li&gt;It only uses ID embedding without item, user characteristics
&lt;ul&gt;
&lt;li&gt;item characteristics: label, key words&lt;/li&gt;
&lt;li&gt;user characteristics: gender, age, geo, liked label&lt;/li&gt;
&lt;li&gt;Two tower Model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Negative sample is wrong
&lt;ul&gt;
&lt;li&gt;Positive: User interacted, which is good&lt;/li&gt;
&lt;li&gt;Negative: User did not interact, which is not correct&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Training
&lt;ul&gt;
&lt;li&gt;dot product is worse than cosine similarity&lt;/li&gt;
&lt;li&gt;Loss(regression) is worse than cross entropy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;implementation&#34;&gt;Implementation&lt;/h4&gt;
&lt;h5 id=&#34;offline&#34;&gt;Offline:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Learning A: user&lt;/li&gt;
&lt;li&gt;Learning B: item&lt;/li&gt;
&lt;li&gt;storing A into {user id: embedding} key-value&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;online&#34;&gt;Online:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;user Id -&amp;gt; find a&amp;rsquo;s vector&lt;/li&gt;
&lt;li&gt;nearest neighbour search, return the top 100 b whose &amp;lt;a, b&amp;gt; is max&lt;/li&gt;
&lt;li&gt;Approximate Nearest Neighbor (ANN)
&lt;ul&gt;
&lt;li&gt;pre process the circile into sectors&lt;/li&gt;
&lt;li&gt;pick a representative vector(indexing) represents the sector&lt;/li&gt;
&lt;li&gt;find representative vector, then find vector in that sector instead of the whole circle&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Recommender System Study Notes - Nomination(1/6)</title>
      <link>http://localhost:1313/post/2024-09-02-rec-sys-nomination-1/</link>
      <pubDate>Sun, 01 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2024-09-02-rec-sys-nomination-1/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Item CF&lt;/li&gt;
&lt;li&gt;Swing&lt;/li&gt;
&lt;li&gt;User CF&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;item-cf&#34;&gt;Item CF&lt;/h3&gt;
&lt;h4 id=&#34;intuition&#34;&gt;Intuition&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;If user likes i1 and i1 is similar with i2 -&amp;gt; Then user is likely to like i2&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;formula&#34;&gt;Formula&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;$score = \sum_j \text{like}(\text{user}, \text{item}\_j) \times \text{sim}(\text{item}\_j, \text{item})$, to compute Item CF score&lt;/li&gt;
&lt;li&gt;$\text{sim}(i_1, i_2) = \frac{|\mathbf{v}|}{\sqrt{|\mathbf{w}_1| \cdot |\mathbf{w}_2|}}$, to compute similarity, w1 as the user who likes i1, w2 as the set, v as the intersection between i1 and i2.&lt;/li&gt;
&lt;li&gt;$\text{sim}(i*1, i_2) = \frac{\sum*{v \in v} \text{like}(v, i*1) \cdot \text{like}(v, i_2)}{\sqrt{\sum*{u*1 \in w_1} \text{like}^2(u_1, i_1)} \cdot \sqrt{\sum*{u_2 \in w_2} \text{like}^2(u_2, i_2)}}$, to take like score into consideration).&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;implementation&#34;&gt;Implementation&lt;/h4&gt;
&lt;h5 id=&#34;offline&#34;&gt;Offline:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;User -&amp;gt; Item Indexing
&lt;ul&gt;
&lt;li&gt;return the last-n items with interaction that this user interacts with.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Item -&amp;gt; Item Indexing
&lt;ul&gt;
&lt;li&gt;compute sim (i, j) maybe maintaining User-&amp;gt;Item&lt;/li&gt;
&lt;li&gt;for each item, find the cloest k items&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;online-inference-user-id---100-items&#34;&gt;Online Inference: (user id -&amp;gt; 100 items)&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;user id -&amp;gt; last-n items&lt;/li&gt;
&lt;li&gt;for each item in last-n, use item-item to find k items&lt;/li&gt;
&lt;li&gt;Use score formula to compute n * k items&amp;rsquo; score&lt;/li&gt;
&lt;li&gt;pick top 100, and output top 100 items as the nomination result in this Item CF Channel&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;swing&#34;&gt;Swing&lt;/h3&gt;
&lt;h4 id=&#34;intuition-1&#34;&gt;Intuition&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;If two items are not that relavant, but it got forwarded into a same small group, we want to lower the similarity score.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;formula-1&#34;&gt;Formula&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;u1 -&amp;gt; j1, u2 -&amp;gt; j2
&lt;ul&gt;
&lt;li&gt;$\text{overlap}(u_1, u_2) = |\mathcal{J}_1 \cap \mathcal{J}_2|$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;i1 -&amp;gt; w1, i2 -&amp;gt; w2, v = w1 ∩ w2
&lt;ul&gt;
&lt;li&gt;$\text{sim}(i_1, i_2) = \sum_{u_1 \in V} \sum_{u_2 \in V} \frac{1}{\alpha + \text{overlap}(u_1, u_2)}$, to compute similarity, a is a hyper-parameter&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;if overlap is big, then the contribution to similarity becomes low&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;user-cf&#34;&gt;User CF&lt;/h3&gt;
&lt;h4 id=&#34;intuition-2&#34;&gt;Intuition&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;I will like the notes which liked by someone who is similar to me.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;formula-2&#34;&gt;Formula&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Method1: like, collect, click notes are similar&lt;/li&gt;
&lt;li&gt;Method2: Subscribed authors are similar&lt;/li&gt;
&lt;li&gt;u1 -&amp;gt; j1, u2 -&amp;gt; j2, I = j1 ∩ j2, nl: the number of users who likes l
&lt;ul&gt;
&lt;li&gt;$sim = \text{sim}(u_1, u_2) = \frac{|I|}{\sqrt{|\mathcal{J}_1| \cdot |\mathcal{J}_2|}}$&lt;/li&gt;
&lt;li&gt;$sim = \text{sim}(u_1, u_2) = \frac{\sum_{I \in I} \frac{1}{\log(1 + n_I)}}{\sqrt{|\mathcal{J}_1| \cdot |\mathcal{J}_2|}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$score = \sum_j \text{sim(user, user}_j\text{)} \times \text{like(user}_j, \text{item)}$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;implementation-1&#34;&gt;Implementation&lt;/h4&gt;
&lt;h5 id=&#34;offline-1&#34;&gt;Offline:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;User -&amp;gt; Item Indexing
&lt;ul&gt;
&lt;li&gt;return the last-n items with interaction that this user interacts with.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;User -&amp;gt; User Indexing
&lt;ul&gt;
&lt;li&gt;compute sim (i, j)&lt;/li&gt;
&lt;li&gt;for each user, find the cloest k users&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;online&#34;&gt;Online:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;user-user indexing: id -&amp;gt; k cloest users&lt;/li&gt;
&lt;li&gt;user-item indexing: for each user, find his/her last-n, use user-item to find n items&lt;/li&gt;
&lt;li&gt;compute score: Use the formula to compute n * k items&amp;rsquo; score&lt;/li&gt;
&lt;li&gt;rank: pick top 100, and output top 100 items as the nomination result in this User CF Channel&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Recommender System Study Notes Basics (1/1)</title>
      <link>http://localhost:1313/post/2024-09-01-rec-sys-basics-1/</link>
      <pubDate>Sun, 01 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2024-09-01-rec-sys-basics-1/</guid>
      <description>&lt;h2 id=&#34;basics&#34;&gt;Basics&lt;/h2&gt;
&lt;div class=&#34;markmap&#34; style=&#34;height: 200px;&#34;&gt;

&lt;pre&gt;- Impression
  - Click
    - ScrollToEnd
      - Comment
    - Like
    - Collect
    - Share&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&#34;基础指标&#34;&gt;基础指标&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Click-through rate = Number of clicks / Number of impressions&lt;/li&gt;
&lt;li&gt;Like rate = Number of likes / Number of impressions&lt;/li&gt;
&lt;li&gt;Collection rate = Number of collections / Number of clicks&lt;/li&gt;
&lt;li&gt;Forwarding rate = Number of forwards / Number of clicks&lt;/li&gt;
&lt;li&gt;Completion rate = Number of times scrolled to the end / Number of clicks * f(Note length)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;北极星指标&#34;&gt;北极星指标&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;用户规模
DAU daily active user
MAU monthly active user&lt;/li&gt;
&lt;li&gt;消费
人均使用时长，人均阅读笔记数量&lt;/li&gt;
&lt;li&gt;发布
发布渗透率，人均发布量&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;实验流程&#34;&gt;实验流程&lt;/h4&gt;
&lt;div class=&#34;markmap&#34; style=&#34;height: 200px;&#34;&gt;

&lt;pre&gt;- Offline Test
  - A/B Test
    - experiment rollout&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&#34;chain-of-recommender-system&#34;&gt;Chain of Recommender System&lt;/h4&gt;
&lt;div class=&#34;markmap&#34; style=&#34;height: 200px;&#34;&gt;

&lt;pre&gt;- Nomination
  - Pre-Ranking
    - Ranking
      - Re-Ranking&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Nomination O(billion) -&amp;gt; O(k)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CF&lt;/li&gt;
&lt;li&gt;Two Tower&lt;/li&gt;
&lt;li&gt;Subscribed Content Creator&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pre-Ranking + Ranking O(k) -&amp;gt; O(100)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use a neural network as follow:&lt;/li&gt;
&lt;li&gt;Input:
&lt;ul&gt;
&lt;li&gt;User Feature Embedding&lt;/li&gt;
&lt;li&gt;Item Feature Embedding&lt;/li&gt;
&lt;li&gt;Stats Embedding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Output:
&lt;ul&gt;
&lt;li&gt;Click-through rate&lt;/li&gt;
&lt;li&gt;Like Rate&lt;/li&gt;
&lt;li&gt;Collection Rate&lt;/li&gt;
&lt;li&gt;Forwarding Rate&lt;/li&gt;
&lt;li&gt;Completion Rate&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Combine them and output a score&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Re-Ranking O(100) -&amp;gt; O(80?) | for diversity&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Common methods:&lt;/li&gt;
&lt;li&gt;MMR&lt;/li&gt;
&lt;li&gt;DPP&lt;/li&gt;
&lt;li&gt;use pre-set rules to diversify notes&lt;/li&gt;
&lt;li&gt;insert ads, change sequence based on rules&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;ab-test&#34;&gt;A/B Test&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Basic flow:
&lt;ul&gt;
&lt;li&gt;new GNN, Offline testing gives promising results&lt;/li&gt;
&lt;li&gt;online A/B test, vaildate&lt;/li&gt;
&lt;li&gt;tune hyper parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;how to put user into buckets randomly
&lt;ul&gt;
&lt;li&gt;hash user id&lt;/li&gt;
&lt;li&gt;compute metrics for each bucket, DAU, etc&lt;/li&gt;
&lt;li&gt;experiment rollout&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Overlapping experiment
&lt;ul&gt;
&lt;li&gt;overlapping: nomination, pre-ranking, ranking, re-ranking, user interface, ads&lt;/li&gt;
&lt;li&gt;each layer: mutually exclusive&lt;/li&gt;
&lt;li&gt;between layers: orthogonality&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Holdout mechanism
&lt;ul&gt;
&lt;li&gt;10% as holdout bucket + 90% as overlapping experiment&lt;/li&gt;
&lt;li&gt;clean holdout and rollout experiment&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Experiment Rollout&lt;/li&gt;
&lt;li&gt;Experiment Reverse&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pre-ranking-rankingscoring&#34;&gt;Pre-Ranking, Ranking(scoring)&lt;/h3&gt;
&lt;h3 id=&#34;re-ranking&#34;&gt;Re-Ranking&lt;/h3&gt;
</description>
    </item>
    
  </channel>
</rss>
